{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b05412",
   "metadata": {},
   "source": [
    "Final flow \n",
    "- all code in functions\n",
    "- data pull from geoserver\n",
    "- column descriptions and layer descriptions from csv\n",
    "- style file from github\n",
    "- output stored locally\n",
    "\n",
    "Common functions between raster and vector wherever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cb8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rasterio\n",
    "import os\n",
    "# import fsspec\n",
    "# import s3fs\n",
    "\n",
    "# import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# from rasterio.warp import transform_bounds\n",
    "\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from shapely.geometry import mapping, box, Polygon\n",
    "\n",
    "import pystac\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c348a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fsspec s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee77845",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOSERVER_BASE_URL = constants.GEOSERVER_BASE_URL\n",
    "# GEOSERVER_BASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6304330",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_DATA_URL = constants.GITHUB_DATA_URL\n",
    "# GITHUB_DATA_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b58b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8d0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_FILE_DIR = os.path.join(LOCAL_DATA_DIR,'input/style_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73be265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "THUMBNAIL_DIR = os.path.join(LOCAL_DATA_DIR,\n",
    "                             'STAC_output_exception_handling')\n",
    "# THUMBNAIL_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24c9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_FILES_DIR = os.path.join(\n",
    "    LOCAL_DATA_DIR,\n",
    "    'CorestackCatalogs_exception_handling' #test folder\n",
    ")\n",
    "\n",
    "# STAC_FILES_DIR = 's3://spatio-temporal-asset-catalog/CorestackCatalogs_prod'\n",
    "\n",
    "# STAC_FILES_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eddceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DESC_GITHUB_URL = constants.LAYER_DESC_GITHUB_URL\n",
    "# LAYER_DESC_GITHUB_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340b7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_COLUMN_DESC_GITHUB_URL = constants.VECTOR_COLUMN_DESC_GITHUB_URL\n",
    "# VECTOR_COLUMN_DESC_GITHUB_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19bb31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_STAC_generated = False #output flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efd831",
   "metadata": {},
   "source": [
    "### Raster flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2773b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_layer_description(filepath,\n",
    "                           layer_name):\n",
    "    if (os.path.exists(filepath)):\n",
    "        layer_desc_df = pd.read_csv(filepath)\n",
    "    else:\n",
    "        #download and save\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        layer_desc_df = pd.read_csv(LAYER_DESC_GITHUB_URL)\n",
    "        layer_desc_df.to_csv(filepath)\n",
    "    if (layer_name in layer_desc_df['layer_name'].tolist()):\n",
    "        layer_desc = layer_desc_df[layer_desc_df['layer_name'] == layer_name]['layer_description'].iloc[0]\n",
    "    else:\n",
    "        print(f\"layer description for {layer_name} layer does not exist currently\")\n",
    "        layer_desc = ''\n",
    "    return layer_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3b16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raster_url(workspace,\n",
    "                        layer_name,\n",
    "                        geoserver_base_url,\n",
    "                        output_format=\"geotiff\"):\n",
    "    wcs_url = (\n",
    "        f\"{geoserver_base_url}/{workspace}/wcs?\"\n",
    "        f\"service=WCS&version=2.0.1&request=GetCoverage&\"\n",
    "        f\"CoverageId={workspace}:{layer_name}&\"\n",
    "        f\"format={output_format}\"\n",
    "    )\n",
    "    print(\"Raster URL:\",wcs_url)\n",
    "    return wcs_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11807658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raster_data(raster_url):\n",
    "\n",
    "    #when reading from geoserver\n",
    "    response = requests.get(raster_url, verify=False)\n",
    "\n",
    "    #exception handling: scenario 1: when fetching data from geoserver\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        # Whoops it wasn't a 200\n",
    "        return \"Error: \" + str(e) + \"when fetching data from geoserver for STAC generation\"\n",
    "\n",
    "    raster_data = BytesIO(response.content)\n",
    "\n",
    "    #read the data and fetch the metadata\n",
    "    with rasterio.open(raster_data) as r:\n",
    "        crs = r.crs\n",
    "        bounds = r.bounds\n",
    "        bbox = [bounds.left, bounds.bottom, bounds.right, bounds.top]\n",
    "        footprint = Polygon([\n",
    "            [bounds.left, bounds.bottom],\n",
    "            [bounds.left, bounds.top],\n",
    "            [bounds.right, bounds.top],\n",
    "            [bounds.right, bounds.bottom]\n",
    "        ])\n",
    "        data = r.read(1) #TODO: wouldn't work if there are multiple bands\n",
    "\n",
    "        # id = os.path.basename(raster_url) #works when data is local\n",
    "        # id = layer_name\n",
    "        # gsd = 10\n",
    "        shape = r.shape\n",
    "        data_type = str(r.dtypes[0])\n",
    "        \n",
    "        return (data,\n",
    "                bbox,\n",
    "                mapping(footprint),\n",
    "                crs,\n",
    "                # id,\n",
    "                # gsd,\n",
    "                shape,\n",
    "                data_type\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e35efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster_item(raster_url,\n",
    "                       id,\n",
    "                       layer_title,\n",
    "                       layer_description,\n",
    "                       start_date='',\n",
    "                       end_date='',\n",
    "                       gsd=pd.NA\n",
    "                       ):\n",
    "\n",
    "    # raster_data,bbox,footprint,crs,id,gsd,shape,data_type = read_raster_data(raster_filepath)\n",
    "    raster_data,bbox,footprint,crs,shape,data_type = read_raster_data(raster_url)\n",
    "\n",
    "    if ((start_date != '') & (end_date != '') & (not pd.isna(gsd))):\n",
    "        raster_item = pystac.Item(id=id,\n",
    "                                  geometry=footprint,\n",
    "                                  bbox=bbox,\n",
    "                                  datetime=datetime.datetime.now(datetime.timezone.utc),\n",
    "                                  properties={\n",
    "                                        \"title\" : layer_title,\n",
    "                                        \"description\" : layer_description,\n",
    "                                        \"start_datetime\": start_date.isoformat() + 'Z',\n",
    "                                        \"end_datetime\": end_date.isoformat() + 'Z',\n",
    "                                      \"gsd\": gsd, #adding this in raster extension \n",
    "                                  })\n",
    "    elif (not pd.isna(gsd)): \n",
    "         raster_item = pystac.Item(id=id,\n",
    "                                  geometry=footprint,\n",
    "                                  bbox=bbox,\n",
    "                                  datetime=datetime.datetime.now(datetime.timezone.utc),\n",
    "                                  properties={\n",
    "                                        \"title\" : layer_title,\n",
    "                                        \"description\" : layer_description,\n",
    "                                      \"gsd\": gsd, #adding this in raster extension \n",
    "                                  })       \n",
    "    else:\n",
    "         raster_item = pystac.Item(id=id,\n",
    "                                  geometry=footprint,\n",
    "                                  bbox=bbox,\n",
    "                                  datetime=datetime.datetime.now(datetime.timezone.utc),\n",
    "                                  properties={\n",
    "                                        \"title\" : layer_title,\n",
    "                                        \"description\" : layer_description\n",
    "                                  })            \n",
    "    \n",
    "    #add certain metadata under projection extension\n",
    "    proj_ext = pystac.extensions.projection.ProjectionExtension.ext(raster_item, add_if_missing=True)\n",
    "    proj_ext.epsg = crs\n",
    "    proj_ext.shape = [shape[0], shape[1]]\n",
    "\n",
    "    return (raster_item,raster_data) #raster_data is needed for thumbnail generation so returning that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c797afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_raster_data_asset(raster_item,\n",
    "                          geoserver_url\n",
    "                          ):\n",
    "    raster_item.add_asset(\"data\", pystac.Asset(\n",
    "        # href=os.path.join(data_url, os.path.relpath(raster_path, start=data_dir)), #TODO\n",
    "        href=geoserver_url,\n",
    "        media_type=pystac.MediaType.GEOTIFF,\n",
    "        roles=[\"data\"],\n",
    "        title=\"Raster Layer\"))\n",
    "\n",
    "    return raster_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d88b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_raster_extension(raster_item): #TODO\n",
    "#         #add certain metadata under raster extension\n",
    "#     raster_ext = pystac.extensions.raster.RasterExtension.ext(raster_item.assets[\"data\"], add_if_missing=True)\n",
    "#     raster_band = pystac.extensions.raster.RasterBand.create(\n",
    "#         data_type=data_type, \n",
    "#         spatial_resolution=gsd,\n",
    "#         # nodata=nodata\n",
    "#     )\n",
    "#     raster_ext.bands = [raster_band]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a64dc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raster_style_file(style_file_url,\n",
    "                            STYLE_FILE_DIR\n",
    "                            ):\n",
    "    \n",
    "    #download style file if not already downloaded, and save it locally\n",
    "    style_file_name = os.path.basename(style_file_url)\n",
    "    style_file_local_path = os.path.join(STYLE_FILE_DIR,\n",
    "                                         style_file_name)\n",
    "    \n",
    "    if not os.path.exists(style_file_local_path):\n",
    "            #TODO: try statement\n",
    "            os.makedirs(os.path.dirname(style_file_local_path), exist_ok=True)\n",
    "            try:\n",
    "                urllib.request.urlretrieve(style_file_url,\n",
    "                                           style_file_local_path)  \n",
    "            #exception handling: scenario 2: when fetching style file from github\n",
    "            except Exception as e:\n",
    "                print(\"Could not retrieve style file from github. Error: \" + str(e))\n",
    "    \n",
    "    tree = ET.parse(style_file_local_path)\n",
    "    root = tree.getroot()\n",
    "    classes = []\n",
    "\n",
    "    for entry in root.findall(\".//paletteEntry\"):\n",
    "        class_info = {}\n",
    "        for attr_key, attr_value in entry.attrib.items():\n",
    "            if attr_key == \"value\":\n",
    "                try:\n",
    "                    class_info[attr_key] = int(attr_value)\n",
    "                except ValueError:\n",
    "                    class_info[attr_key] = attr_value\n",
    "            else:\n",
    "                class_info[attr_key] = attr_value\n",
    "        classes.append(class_info)\n",
    "\n",
    "    # If no paletteEntry tags are found, check for item tags\n",
    "    if not classes:\n",
    "        for entry in root.findall(\".//item\"):\n",
    "            class_info = {}\n",
    "            for attr_key, attr_value in entry.attrib.items():\n",
    "                if attr_key == \"value\":\n",
    "                    try:\n",
    "                        class_info[attr_key] = int(attr_value)\n",
    "                    except ValueError:\n",
    "                        class_info[attr_key] = attr_value\n",
    "                else:\n",
    "                    class_info[attr_key] = attr_value\n",
    "            classes.append(class_info)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee2a3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_classification_extension(raster_style_url,\n",
    "                                 raster_item,\n",
    "                                 STYLE_FILE_DIR\n",
    "                                 ):\n",
    "    \n",
    "    style_info = parse_raster_style_file(style_file_url=raster_style_url,\n",
    "                                         STYLE_FILE_DIR=STYLE_FILE_DIR\n",
    "                                         )\n",
    "    classification_ext = pystac.extensions.classification.ClassificationExtension.ext(raster_item.assets[\"data\"], add_if_missing=True)\n",
    "    stac_classes = []\n",
    "    for cls in style_info:\n",
    "        stac_class_obj = pystac.extensions.classification.Classification.create(\n",
    "            value=int(cls[\"value\"]),\n",
    "            name=cls.get(\"label\") or f\"Class {cls['value']}\",\n",
    "            description=cls.get(\"label\"),\n",
    "            color_hint=cls['color'].replace('#','')\n",
    "        )\n",
    "        stac_classes.append(stac_class_obj)\n",
    "    classification_ext.classes = stac_classes\n",
    "\n",
    "    return (raster_item,style_info) #style info is required for thumbnail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c52c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stylefile_asset(STAC_item,\n",
    "                        style_file_url):\n",
    "    STAC_item.add_asset(\"style\", pystac.Asset(\n",
    "        # href=os.path.join(data_url, os.path.relpath(raster_style_path, start=data_dir)),\n",
    "        href=style_file_url,\n",
    "        media_type=pystac.MediaType.XML,\n",
    "        roles=[\"metadata\"],\n",
    "        title=\"QGIS Style file\"\n",
    "    ))\n",
    "    return STAC_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "732f5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raster_thumbnail(raster_data,\n",
    "                              style_info,\n",
    "                              output_path\n",
    "                              ):\n",
    "    \n",
    "    unique_raster_values = np.unique(raster_data.compressed() if isinstance(raster_data, np.ma.MaskedArray) else raster_data)\n",
    "    # Filter QML info to only include values present in the raster data\n",
    "    filtered_style_info = [cls for cls in style_info if cls.get('value') in unique_raster_values]\n",
    "    \n",
    "    values = [cls['value'] for cls in filtered_style_info if 'value' in cls]\n",
    "    colors = [cls['color'] for cls in filtered_style_info if 'color' in cls]\n",
    "    \n",
    "    # print(f\"Parsed QML values: {values}\")\n",
    "    # print(f\"Parsed QML colors: {colors}\")\n",
    "        \n",
    "    try:\n",
    "        if not values or not colors or len(values) != len(colors):\n",
    "            raise ValueError(\"Invalid or insufficient palette information in QML file.\")\n",
    "    \n",
    "        sorted_indices = np.argsort(values)\n",
    "        sorted_values = np.array(values)[sorted_indices]\n",
    "        sorted_colors = np.array(colors)[sorted_indices]\n",
    "\n",
    "        cmap = ListedColormap(sorted_colors)\n",
    "        bounds = np.array(sorted_values) - 0.5\n",
    "        bounds = np.append(bounds, sorted_values[-1] + 0.5)\n",
    "        norm = Normalize(vmin=bounds.min(), vmax=bounds.max())\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping palette generation due to error: {e}. Using a default colormap.\")\n",
    "        cmap = 'gray'\n",
    "        norm = None\n",
    "    plt.figure(figsize=(3, 3), dpi=100)\n",
    "    \n",
    "    plt.imshow(raster_data, cmap=cmap, norm=norm, interpolation='none')\n",
    "    plt.axis('off')\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dd6bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thumbnail_asset(STAC_item,\n",
    "                        THUMBNAIL_PATH,\n",
    "                        LOCAL_DATA_DIR, #TODO\n",
    "                        GITHUB_DATA_URL\n",
    "                        ):\n",
    "    STAC_item.add_asset(\"thumbnail\", pystac.Asset(\n",
    "        href=os.path.join(GITHUB_DATA_URL, os.path.relpath(THUMBNAIL_PATH,\n",
    "                                                           start=LOCAL_DATA_DIR)),\n",
    "        media_type=pystac.MediaType.PNG,\n",
    "        roles=[\"thumbnail\"],\n",
    "        title=\"Thumbnail\"\n",
    "    ))\n",
    "\n",
    "    return STAC_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3236da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_layer_mapping(layer_map_csv_path, #TODO: update this function for each type of layer\n",
    "                       layer_name,\n",
    "                       district, #\n",
    "                       block,\n",
    "                       start_year = '',\n",
    "                       end_year = ''\n",
    "                       ):\n",
    "    layer_mapping_df = pd.read_csv(layer_map_csv_path)\n",
    "    layer_display_name = layer_mapping_df[layer_mapping_df['layer_name'] == layer_name]['display_name'].iloc[0]\n",
    "\n",
    "    geoserver_workspace_name = layer_mapping_df[layer_mapping_df['layer_name'] == layer_name]['geoserver_workspace_name'].iloc[0]\n",
    "    geoserver_layer_name = layer_mapping_df[layer_mapping_df['layer_name'] == layer_name]['geoserver_layer_name'].iloc[0]\n",
    "    \n",
    "    style_file_url = layer_mapping_df[layer_mapping_df['layer_name'] == layer_name]['style_file_url'].iloc[0]\n",
    "    ee_layer_name = layer_mapping_df[layer_mapping_df['layer_name'] == layer_name]['ee_layer_name'].iloc[0]\n",
    "\n",
    "    gsd = layer_mapping_df[layer_mapping_df['layer_name'] == layer_name]['spatial_resolution_in_meters'].iloc[0]\n",
    "    \n",
    "    if (layer_name == 'land_use_land_cover_raster'):\n",
    "        start_year = str(int(start_year) % 100) #keep only last 2 digits of the full year\n",
    "        end_year = str(int(end_year) % 100)\n",
    "        geoserver_layer_name = geoserver_layer_name.format(start_year = start_year,\n",
    "                                                    end_year = end_year,\n",
    "                                                    block = block)    \n",
    "    # print(geoserver_workspace_name,geoserver_layer_name)\n",
    "    elif ((layer_name == 'tree_canopy_cover_density_raster') |\\\n",
    "          (layer_name == 'tree_canopy_height_raster')):\n",
    "                geoserver_layer_name = geoserver_layer_name.format(start_year = start_year,\n",
    "                                                                   district = district,\n",
    "                                                                   block = block)\n",
    "    else: #only LULC has some specific things such as 2 digit year representation and no district information\n",
    "        geoserver_layer_name = geoserver_layer_name.format(district = district,\n",
    "                                                           block = block)\n",
    "    return (geoserver_workspace_name,\n",
    "            geoserver_layer_name,\n",
    "            style_file_url,\n",
    "            layer_display_name,\n",
    "            ee_layer_name,\n",
    "            gsd\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adba05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raster_item(state,\n",
    "                         district,\n",
    "                         block,\n",
    "                         layer_name,\n",
    "                         layer_map_csv_path,\n",
    "                         layer_desc_csv_path,\n",
    "                         start_year,\n",
    "                         end_year\n",
    "                         ):    \n",
    "    \n",
    "    #1. read layer description\n",
    "    layer_description = read_layer_description(filepath= layer_desc_csv_path,\n",
    "                                               layer_name=layer_name)\n",
    "    \n",
    "    #2. get geoserver url parameters from the layer details\n",
    "    geoserver_workspace_name,geoserver_layer_name,style_file_url,layer_display_name,ee_layer_name,gsd = \\\n",
    "        read_layer_mapping(layer_map_csv_path = layer_map_csv_path,\n",
    "                           district = district,\n",
    "                           block=block,\n",
    "                           layer_name=layer_name,\n",
    "                           start_year=start_year,\n",
    "                           end_year=end_year\n",
    "                           )\n",
    "    print(f\"geoserver_workspace_name={geoserver_workspace_name}\")\n",
    "    print(f\"geoserver_layer_name={geoserver_layer_name}\")\n",
    "    print(f\"style file url = {style_file_url}\")\n",
    "\n",
    "    #3. generate geoserver url\n",
    "    geoserver_url = generate_raster_url(workspace=geoserver_workspace_name,\n",
    "                                        layer_name=geoserver_layer_name,\n",
    "                                        geoserver_base_url=GEOSERVER_BASE_URL)\n",
    "    \n",
    "    #4. create raster item\n",
    "    #updated layer title and layer id\n",
    "    start_date = ''\n",
    "    end_date = ''\n",
    "    layer_title = layer_display_name #default layer title\n",
    "    layer_id = f\"{state}_{district}_{block}_{layer_name}\" #default layer id\n",
    "\n",
    "    #Update it further if start year and end year exist\n",
    "    if (start_year != \"\"):\n",
    "        layer_title = f\"{layer_display_name} : {start_year}\"\n",
    "        layer_id = f\"{state}_{district}_{block}_{layer_name}_{start_year}\"\n",
    "        start_date = start_year + '-' + constants.AGRI_YEAR_START_DATE\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        if (end_year == ''): #like in tree layers\n",
    "            end_date = str(int(start_year) + 1) + '-' + constants.AGRI_YEAR_END_DATE\n",
    "            end_date = pd.to_datetime(end_date)\n",
    "    if (end_year != \"\"):\n",
    "        end_date = end_year + '-' + constants.AGRI_YEAR_END_DATE\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "    print(f\"start_date = {start_date}\")\n",
    "    print(f\"end_date = {end_date}\")\n",
    "    raster_item,raster_data = create_raster_item(geoserver_url,\n",
    "                                                #  id=f\"{layer_name}_{block}\",\n",
    "                                                #  id=geoserver_layer_name,\n",
    "                                                 id=layer_id,\n",
    "                                                 layer_title=layer_title,\n",
    "                                                 layer_description = layer_description,\n",
    "                                                 start_date=start_date,\n",
    "                                                 end_date=end_date,\n",
    "                                                 gsd = gsd\n",
    "                                                 )\n",
    "    \n",
    "    #5. add raster data asset\n",
    "    raster_item = add_raster_data_asset(raster_item,\n",
    "                                        geoserver_url=geoserver_url)\n",
    "    \n",
    "    #6. add classification extension\n",
    "    raster_item,style_info = add_classification_extension(raster_style_url=style_file_url,\n",
    "                                                          raster_item=raster_item,\n",
    "                                                          STYLE_FILE_DIR=STYLE_FILE_DIR\n",
    "                                                          )\n",
    "    \n",
    "    #7. add style file asset\n",
    "    add_stylefile_asset(STAC_item=raster_item,\n",
    "                        style_file_url=style_file_url\n",
    "                        )\n",
    "    \n",
    "    #8. generate thumbnail\n",
    "    if (start_year != ''):\n",
    "        thumbnail_filename = f'{block}_{layer_name}_{start_year}.png'\n",
    "    else:\n",
    "        thumbnail_filename = f'{block}_{layer_name}.png' #TODO:\n",
    "    THUMBNAIL_PATH = os.path.join(THUMBNAIL_DIR,\n",
    "                                         thumbnail_filename)\n",
    "    \n",
    "    generate_raster_thumbnail(raster_data=raster_data,\n",
    "                              style_info=style_info,\n",
    "                              output_path=THUMBNAIL_PATH\n",
    "                              )\n",
    "    \n",
    "    #9. add thumbnail asset\n",
    "    raster_item = add_thumbnail_asset(\n",
    "        STAC_item=raster_item,\n",
    "        THUMBNAIL_PATH=THUMBNAIL_PATH,\n",
    "        LOCAL_DATA_DIR=LOCAL_DATA_DIR,\n",
    "        GITHUB_DATA_URL=GITHUB_DATA_URL\n",
    "    )\n",
    "\n",
    "    return raster_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5673865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = fsspec.filesystem('s3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23b64fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_STAC_files(state,\n",
    "                      district,\n",
    "                      block,\n",
    "                      STAC_item,\n",
    "                    #   corestack_dir,#TODO\n",
    "                    #   block_title,#TODO\n",
    "                    #   district_title,#TODO\n",
    "                    #   state_title #TODO\n",
    "                      ):\n",
    "    \n",
    "    # def s3_path(*args):\n",
    "    #     return os.path.join(*args).replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    #1. create block catalog,if not already existing \n",
    "    # block_dir = s3_path(STAC_FILES_DIR,\n",
    "    #                          state,\n",
    "    #                          district,\n",
    "    #                          block)\n",
    "    block_dir = os.path.join(STAC_FILES_DIR,\n",
    "                             state,\n",
    "                             district,\n",
    "                             block)\n",
    "    os.makedirs(block_dir, exist_ok=True)\n",
    "    block_catalog_path = os.path.join(block_dir,'catalog.json')\n",
    "    # block_catalog_path = s3_path(block_dir,'catalog.json')\n",
    "\n",
    "    #if it already exists, read the catalog\n",
    "    if os.path.exists(block_catalog_path):\n",
    "    # if fs.exists(block_catalog_path):  \n",
    "        block_catalog = pystac.read_file(block_catalog_path)\n",
    "        print(f\"Loaded existing block catalog: {block}\")\n",
    "\n",
    "        #If block exists already then also district,state and root exist so nothing needs to be done further! \n",
    "        #just save the updated block \n",
    "         #2. add item to block catalog. \n",
    "\n",
    "        #currently if the item already exists in the catalog, remove it, and add the newly generated\n",
    "        #item. \n",
    "        block_items = list(block_catalog.get_all_items())\n",
    "        if (STAC_item.id in [x.id for x in block_items]):\n",
    "            #remove older entry\n",
    "            block_catalog.remove_item(STAC_item.id)\n",
    "            print(f\"Removed previously existing {STAC_item.id} from the catalog\")\n",
    "\n",
    "        block_catalog.add_item(STAC_item)\n",
    "        block_catalog.normalize_and_save(block_dir,\n",
    "                                         catalog_type=pystac.CatalogType.SELF_CONTAINED)\n",
    "        return \n",
    "    else:\n",
    "        block_catalog = pystac.Catalog(\n",
    "            id=block,\n",
    "            title=f\"STAC for {block}\",\n",
    "            description=f\"STAC catalog for {block} block data in {district}, {state}\")\n",
    "\n",
    "        print(\"created block catalog\")\n",
    "\n",
    "    #2. add item to block catalog. \n",
    "    block_catalog.add_item(STAC_item)\n",
    "    \n",
    "    #3. create district catalog if not existing\n",
    "    district_dir = os.path.join(STAC_FILES_DIR,\n",
    "                             state,\n",
    "                             district)\n",
    "    os.makedirs(district_dir, exist_ok=True)\n",
    "    district_catalog_path = os.path.join(district_dir,'catalog.json')\n",
    "\n",
    "    ##if os.path.exists(district_catalog_path):\n",
    "    if os.path.exists(district_catalog_path):\n",
    "        district_catalog = pystac.read_file(district_catalog_path)\n",
    "        print(\"loaded district catalog\")\n",
    "        #add block catalog to the district\n",
    "        district_catalog.add_child(block_catalog)\n",
    "        district_catalog.normalize_and_save(district_dir, catalog_type=pystac.CatalogType.SELF_CONTAINED)\n",
    "        return\n",
    "    else: \n",
    "        district_catalog = pystac.Catalog(\n",
    "            id=district,\n",
    "            title= f\"{district}\",#f\"{district_title}\",\n",
    "            description=f\"STAC catalog for data of {district} district\"\n",
    "        )   \n",
    "        print(\"created district catalog\")\n",
    "        #add block catalog to the district\n",
    "        district_catalog.add_child(block_catalog)\n",
    "        district_catalog.normalize_and_save(district_dir, catalog_type=pystac.CatalogType.SELF_CONTAINED)     \n",
    "\n",
    "    #4. create state collection if not existing\n",
    "    state_dir = os.path.join(STAC_FILES_DIR,\n",
    "                             state)\n",
    "    state_collection_path = os.path.join(state_dir, \"collection.json\")\n",
    "\n",
    "    if os.path.exists(state_collection_path):\n",
    "    # if fs.exists(state_collection_path):    \n",
    "        state_collection = pystac.read_file(state_collection_path)\n",
    "        print(\"loaded state collection\")\n",
    "        state_collection.add_child(district_catalog)\n",
    "        state_collection.normalize_and_save(state_dir, catalog_type=pystac.CatalogType.SELF_CONTAINED)\n",
    "        return \n",
    "    else:\n",
    "        state_collection = pystac.Collection(\n",
    "            id=state,\n",
    "            title= state,#f\"{state_title}\",\n",
    "            description=f\"STAC Collection for data of {state} state.\",\n",
    "            license=\"CC-BY-4.0\",\n",
    "            extent=pystac.Extent(\n",
    "                spatial=pystac.SpatialExtent([0, 0, 0, 0]), #TODO: let it come automatically from data, or make it India wide\n",
    "                temporal=pystac.TemporalExtent([[constants.DEFAULT_START_DATE, \n",
    "                                                 constants.DEFAULT_END_DATE]])),\n",
    "            providers=[ \n",
    "                pystac.Provider(\n",
    "                    name=\"CoRE Stack\",\n",
    "                    roles=[pystac.ProviderRole.PRODUCER, pystac.ProviderRole.PROCESSOR, pystac.ProviderRole.HOST, pystac.ProviderRole.LICENSOR ],\n",
    "                    url=\"https://core-stack.org/\"\n",
    "                    )],\n",
    "                    keywords=[\"social-ecological\", \"sustainability\",\"CoRE stack\", block, district, state]\n",
    "                    )   #TODO: move this to constants and not hardcode it here. \n",
    "        \n",
    "          \n",
    "        state_collection.add_link(\n",
    "            pystac.Link(\n",
    "                rel=pystac.RelType.LICENSE,\n",
    "                target=\"https://spdx.org/licenses/CC-BY-4.0.html\",\n",
    "                media_type=\"text/html\")\n",
    "        )\n",
    "         \n",
    "        state_collection.add_link(\n",
    "            pystac.Link(\n",
    "                rel=\"documentation\",\n",
    "                target=\"https://core-stack.org/\",\n",
    "                title=\"CoRE stack\",\n",
    "                media_type=\"application/pdf\")\n",
    "        )\n",
    "\n",
    "        state_collection.add_link(\n",
    "            pystac.Link(\n",
    "                rel=\"documentation\",\n",
    "                target=\"https://drive.google.com/file/d/1ZxovdpPThkN09cB1TcUYSE2BImI7M3k_/view\",\n",
    "                title=\"Technical Manual\",\n",
    "                media_type=\"application/pdf\")\n",
    "        )\n",
    "        \n",
    "        state_collection.add_link(\n",
    "            pystac.Link(\n",
    "                rel=\"documentation\",\n",
    "                target=\"https://github.com/orgs/core-stack-org/repositories\",\n",
    "                title=\"Github link\",\n",
    "                media_type=\"application/pdf\")\n",
    "        ) \n",
    "\n",
    "        print(\"created state collection\")\n",
    "        state_collection.add_child(district_catalog)\n",
    "        state_collection.normalize_and_save(state_dir, catalog_type=pystac.CatalogType.SELF_CONTAINED)\n",
    "        \n",
    "    #5. create root catalog if not existing\n",
    "    root_catalog_path = os.path.join(STAC_FILES_DIR, \"catalog.json\")\n",
    "    if os.path.exists(root_catalog_path):\n",
    "    # if fs.exists(root_catalog_path):   \n",
    "        root_catalog = pystac.read_file(root_catalog_path)\n",
    "        print(\"loaded root catalog\")\n",
    "    else:\n",
    "        ##os.makedirs(STAC_FILES_DIR, exist_ok=True)\n",
    "        root_catalog = pystac.Catalog(\n",
    "            id=\"corestack_STAC\",\n",
    "            title=constants.ROOT_CATALOG_TITLE,\n",
    "            description=constants.ROOT_CATALOG_DESCRIPTION\n",
    "        )\n",
    "        ##root_catalog.set_self_href(root_catalog_path)\n",
    "        print(\"created root catalog\")\n",
    "    root_catalog.add_child(state_collection)\n",
    "    root_catalog.normalize_and_save(STAC_FILES_DIR, catalog_type=pystac.CatalogType.SELF_CONTAINED)\n",
    "    layer_STAC_generated = True \n",
    "    return layer_STAC_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d2811",
   "metadata": {},
   "source": [
    "### Vector flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72f6ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_url(workspace,\n",
    "                        layer_name,\n",
    "                        geoserver_base_url,\n",
    "                        output_format=\"json\"):\n",
    "    # wfs_url = (\n",
    "    #     f\"{geoserver_base_url}/{workspace}/ows?\"\n",
    "    #     f\"service=WFS&version=1.0.0&request=GetFeature&\"\n",
    "    #     f\"typeName={workspace}:{layer_name}&\"\n",
    "    #     f\"outputFormat==application/json\"\n",
    "    # )\n",
    "\n",
    "    wfs_url = (\n",
    "        f\"{geoserver_base_url}/{workspace}/ows?\"\n",
    "        f\"service=WFS&version=1.0.0&request=GetFeature&\"\n",
    "        f\"typeName={workspace}:{layer_name}&outputFormat=application/json\")\n",
    "    # print(\"Vector URL:\",wfs_url)\n",
    "\n",
    "    return wfs_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75432d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vector_data(vector_url,\n",
    "                     target_crs='4326'\n",
    "                     ):\n",
    "    try:\n",
    "        vector_gdf = gpd.read_file(vector_url)\n",
    "    except Exception as e:\n",
    "        print(\"Could not fetch vector data from url. Error: \" + str(e))\n",
    "\n",
    "    vector_gdf = vector_gdf.to_crs(epsg=target_crs)\n",
    "    #TODO: remove such constants like here in crs. make it standard. available in constants. \n",
    "    bounds = vector_gdf.total_bounds\n",
    "    bbox = [float(b) for b in bounds] #footprint also in vector\n",
    "    geom = mapping(vector_gdf.union_all())    \n",
    "    return (vector_gdf,bounds,bbox,geom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a1f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_item(vector_url,\n",
    "                       id,\n",
    "                       layer_title,\n",
    "                       layer_description,\n",
    "                       column_desc_csv_path\n",
    "                       ):\n",
    "    vector_gdf, bounds, bbox, geom = read_vector_data(vector_url=vector_url)\n",
    "\n",
    "    vector_item = pystac.Item(\n",
    "        id=id,\n",
    "        geometry=geom,\n",
    "        bbox=bbox,\n",
    "        datetime=datetime.datetime.now(datetime.timezone.utc),\n",
    "        properties={\n",
    "            \"title\": layer_title,\n",
    "            \"description\": layer_description,\n",
    "            # \"start_datetime\": start_date.isoformat() + 'Z',\n",
    "            # \"end_datetime\": end_date.isoformat() + 'Z',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return vector_item,vector_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "338feccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vector_data_asset(vector_item,\n",
    "                          geoserver_url\n",
    "                          ):\n",
    "                          \n",
    "    vector_item.add_asset(\"data\", pystac.Asset(\n",
    "        href=geoserver_url,\n",
    "        media_type=pystac.MediaType.GEOJSON,\n",
    "        roles=[\"data\"],\n",
    "        title=\"Vector Layer\"))\n",
    "    \n",
    "    return vector_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf5e3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tabular_extension(vector_item,\n",
    "                          vector_data_gdf,\n",
    "                          column_desc_csv_path,\n",
    "                          ee_layer_name\n",
    "                          ):\n",
    "    if (os.path.exists(column_desc_csv_path)):\n",
    "        vector_column_desc_gdf = pd.read_csv(column_desc_csv_path)\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(column_desc_csv_path), exist_ok=True)\n",
    "        vector_column_desc_gdf = pd.read_csv(VECTOR_COLUMN_DESC_GITHUB_URL)\n",
    "        vector_column_desc_gdf.to_csv(column_desc_csv_path)\n",
    "\n",
    "    vector_column_desc_filtered_gdf = vector_column_desc_gdf[vector_column_desc_gdf['ee_layer_name'] == ee_layer_name]\n",
    "    vector_column_desc_filtered_gdf.rename({'column_name_description':'column_description'},axis=1,inplace=True)\n",
    "    table_ext = pystac.extensions.table.TableExtension.ext(vector_item, add_if_missing=True)\n",
    "    vector_merged_df = vector_data_gdf.dtypes.reset_index()\n",
    "    vector_merged_df.columns = ['column_name','column_dtype']\n",
    "    vector_merged_df = vector_merged_df.merge(vector_column_desc_filtered_gdf[['column_name','column_description']],\n",
    "                                            on='column_name',\n",
    "                                            how='left').fillna('')\n",
    "    \n",
    "    table_ext.columns = [\n",
    "        {\n",
    "            \"name\": row['column_name'],\n",
    "            \"type\": str(row['column_dtype']),\n",
    "            \"description\" : row['column_description']\n",
    "        }\n",
    "        for ind,row in vector_merged_df.iterrows()\n",
    "        ]\n",
    "    \n",
    "    return vector_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fba68d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to generate vector thumbnail \n",
    "def rgba_to_hex(rgba_tuple):\n",
    "    if rgba_tuple is None:\n",
    "        return '#808080'  # Default gray\n",
    "    r, g, b, a = rgba_tuple\n",
    "    return f\"#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}\"\n",
    "\n",
    "\n",
    "def extract_styling_info(symbol_element):\n",
    "    fill_color = None\n",
    "    outline_color = None\n",
    "    line_width = None\n",
    "\n",
    "    if symbol_element is None:\n",
    "        return fill_color, outline_color, line_width\n",
    "\n",
    "    \n",
    "    fill_layer = symbol_element.find('.//layer[@class=\"SimpleFill\"]')\n",
    "    if fill_layer is not None:\n",
    "        color_option = fill_layer.find('Option[@name=\"color\"]')\n",
    "        if color_option is not None:\n",
    "            try:\n",
    "                rgb_parts = [int(p) for p in color_option.get('value').split(',')[:3]]\n",
    "                fill_color = tuple([p / 255 for p in rgb_parts])\n",
    "            except (ValueError, TypeError):\n",
    "                fill_color = None\n",
    "\n",
    "        outline_option = fill_layer.find('Option[@name=\"outline_color\"]')\n",
    "        if outline_option is not None:\n",
    "            try:\n",
    "                rgb_parts = [int(p) for p in outline_option.get('value').split(',')[:3]]\n",
    "                outline_color = tuple([p / 255 for p in rgb_parts])\n",
    "            except (ValueError, TypeError):\n",
    "                outline_color = None\n",
    "        \n",
    "        width_option = fill_layer.find('Option[@name=\"outline_width\"]')\n",
    "        if width_option is not None:\n",
    "            try:\n",
    "                line_width = float(width_option.get('value'))\n",
    "            except (ValueError, TypeError):\n",
    "                line_width = None\n",
    "\n",
    "    \n",
    "    line_layer = symbol_element.find('.//layer[@class=\"SimpleLine\"]')\n",
    "    if line_layer is not None:\n",
    "        color_option = line_layer.find('Option[@name=\"line_color\"]')\n",
    "        if color_option is not None:\n",
    "            try:\n",
    "                rgb_parts = [int(p) for p in color_option.get('value').split(',')[:3]]\n",
    "                outline_color = tuple([p / 255 for p in rgb_parts])\n",
    "            except (ValueError, TypeError):\n",
    "                outline_color = None\n",
    "        \n",
    "        width_option = line_layer.find('Option[@name=\"line_width\"]')\n",
    "        if width_option is not None:\n",
    "            try:\n",
    "                line_width = float(width_option.get('value'))\n",
    "            except (ValueError, TypeError):\n",
    "                line_width = None\n",
    "    \n",
    "    return fill_color, outline_color, line_width\n",
    "\n",
    "def parse_vector_style_file(style_file_url,\n",
    "                            STYLE_FILE_DIR\n",
    "                            ):\n",
    "    #download style file if not already downloaded, and save it locally\n",
    "    style_file_name = os.path.basename(style_file_url)\n",
    "    style_file_local_path = os.path.join(STYLE_FILE_DIR,\n",
    "                                         style_file_name)\n",
    "    \n",
    "    if not os.path.exists(style_file_local_path):\n",
    "            #TODO: try statement\n",
    "            os.makedirs(os.path.dirname(style_file_local_path), exist_ok=True)\n",
    "            try:\n",
    "                urllib.request.urlretrieve(style_file_url,\n",
    "                                           style_file_local_path)\n",
    "            except Exception as e:\n",
    "                print(\"Could not retrieve style file from url. Error: \" + str(e))\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(style_file_local_path)\n",
    "        root = tree.getroot()\n",
    "        renderer_element = root.find('.//renderer-v2')\n",
    "\n",
    "        if renderer_element is None:\n",
    "            print(\"No renderer-v2 element found.\")\n",
    "            return None\n",
    "\n",
    "        renderer_type = renderer_element.get('type')\n",
    "        style = {'renderer_type': renderer_type}\n",
    "        symbols = {s.get('name'): s for s in root.findall('.//symbols/symbol')}\n",
    "\n",
    "        if renderer_type == 'singleSymbol':\n",
    "            symbol_element = renderer_element.find('.//symbol') or symbols.get(renderer_element.get('symbol'))\n",
    "            if symbol_element is not None:\n",
    "                \n",
    "                color_option = symbol_element.find('.//layer/Option[@name=\"line_color\"]') or symbol_element.find('.//layer/Option[@name=\"color\"]')\n",
    "                \n",
    "                if color_option is not None:\n",
    "                    color_value = color_option.get('value').split(',')[0:3]\n",
    "                    rgb_parts = [int(p) for p in color_value]\n",
    "                    style['color'] = (rgb_parts[0] / 255, rgb_parts[1] / 255, rgb_parts[2] / 255)\n",
    "                else:\n",
    "                    \n",
    "                    color_prop = symbol_element.find('.//prop[@k=\"color\"]')\n",
    "                    if color_prop is not None:\n",
    "                        rgb_parts = [int(p) for p in color_prop.get('v').split(',')[:3]]\n",
    "                        style['color'] = (rgb_parts[0] / 255, rgb_parts[1] / 255, rgb_parts[2] / 255)\n",
    "                    else:\n",
    "                        print(f\"Warning: Single symbol color not found in {style_file_local_path}.\")\n",
    "                        return None\n",
    "            else:\n",
    "                print(f\"Warning: Could not find symbol element for singleSymbol in {style_file_local_path}.\")\n",
    "                return None\n",
    "\n",
    "        elif renderer_type == 'categorizedSymbol':\n",
    "            style['attribute'] = renderer_element.get('attr')\n",
    "            style['categories'] = []\n",
    "            for cat in renderer_element.findall('categories/category'):\n",
    "                symbol_element = cat.find('symbol') or symbols.get(cat.get('symbol'))\n",
    "                fill_color, outline_color, line_width = extract_styling_info(symbol_element)\n",
    "                style['categories'].append({\n",
    "                    'value': cat.get('value'),\n",
    "                    'label': cat.get('label'),\n",
    "                    'fill_color': fill_color,\n",
    "                    'outline_color': outline_color,\n",
    "                    'line_width': line_width\n",
    "                })\n",
    "\n",
    "        elif renderer_type == 'graduatedSymbol':\n",
    "            style['attribute'] = renderer_element.get('attr')\n",
    "            style['classes'] = []\n",
    "            for cls in renderer_element.findall('classes/class'):\n",
    "                symbol_element = cls.find('symbol') or symbols.get(cls.get('symbol'))\n",
    "                fill_color, outline_color, line_width = extract_styling_info(symbol_element)\n",
    "                style['classes'].append({\n",
    "                    'lower_bound': float(cls.get('lower')),\n",
    "                    'upper_bound': float(cls.get('upper')),\n",
    "                    'label': cls.get('label'),\n",
    "                    'fill_color': fill_color,\n",
    "                    'outline_color': outline_color,\n",
    "                    'line_width': line_width\n",
    "                })\n",
    "        \n",
    "        elif renderer_type == 'RuleRenderer':\n",
    "            style['rules'] = []\n",
    "            for rule in renderer_element.findall('.//rule'):\n",
    "                symbol_element = rule.find('.//symbol')\n",
    "                fill_color, outline_color, line_width = extract_styling_info(symbol_element)\n",
    "                style['rules'].append({\n",
    "                    'filter': rule.get('filter'),\n",
    "                    'label': rule.get('label'),\n",
    "                    'fill_color': fill_color,\n",
    "                    'outline_color': outline_color,\n",
    "                    'line_width': line_width\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Warning: Unsupported renderer type '{renderer_type}'. Using default style.\")\n",
    "            return None\n",
    "        return style\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing QML file {style_file_local_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "057f9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_thumbnail(vector_gdf,\n",
    "                              style_file_url,\n",
    "                              output_path,\n",
    "                              STYLE_FILE_DIR\n",
    "                              ):\n",
    "    \n",
    "    try:\n",
    "        # vector_gdf = gpd.read_file(vector_path)\n",
    "        style_info = parse_vector_style_file(style_file_url,STYLE_FILE_DIR)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "        default_fill_color = (0.8, 0.8, 0.8, 1.0) # Light gray\n",
    "        default_outline_color = (0, 0, 0, 1.0)   # Black\n",
    "        default_line_width = 1.0\n",
    "\n",
    "        if style_info is None:\n",
    "            print(\"Applying default style due to parsing error.\")\n",
    "            vector_gdf.plot(ax=ax,\n",
    "                            color=rgba_to_hex(default_fill_color), \n",
    "                            edgecolor=rgba_to_hex(default_outline_color),\n",
    "                            linewidth=default_line_width)\n",
    "        \n",
    "        elif style_info.get('renderer_type') == 'singleSymbol':\n",
    "            print(\"Applying single symbol style...\")\n",
    "            fill_color = style_info.get('fill_color', default_fill_color)\n",
    "            outline_color = style_info.get('outline_color', default_outline_color)\n",
    "            line_width = style_info.get('line_width', default_line_width)\n",
    "            vector_gdf.plot(ax=ax,\n",
    "                            color=rgba_to_hex(fill_color),\n",
    "                            edgecolor=rgba_to_hex(outline_color),\n",
    "                            linewidth=line_width)\n",
    "\n",
    "        elif style_info.get('renderer_type') == 'categorizedSymbol':\n",
    "            print(\"Applying categorized style...\")\n",
    "            \n",
    "            color_map = {\n",
    "                cat.get('value'): rgba_to_hex(cat.get('fill_color', default_fill_color))\n",
    "                for cat in style_info.get('categories', [])\n",
    "            }\n",
    "            \n",
    "            outline_color_map = {\n",
    "                cat.get('value'): rgba_to_hex(cat.get('outline_color', default_outline_color))\n",
    "                for cat in style_info.get('categories', [])\n",
    "            }\n",
    "\n",
    "            attribute_name = style_info.get('attribute')\n",
    "\n",
    "            if attribute_name not in vector_gdf.columns:\n",
    "                print(f\"Error: Attribute column '{attribute_name}' not found. Applying default style.\")\n",
    "                vector_gdf.plot(ax=ax,\n",
    "                                color=rgba_to_hex(default_fill_color),\n",
    "                                edgecolor=rgba_to_hex(default_outline_color),\n",
    "                                linewidth=default_line_width)\n",
    "            else:\n",
    "                vector_gdf['mapped_value'] = vector_gdf[attribute_name].apply(lambda x: str(x).strip() if pd.notnull(x) else None)\n",
    "                \n",
    "                fill_colors = vector_gdf['mapped_value'].map(color_map)\n",
    "                fill_colors = fill_colors.fillna(rgba_to_hex(default_fill_color))\n",
    "\n",
    "                outline_colors = vector_gdf['mapped_value'].map(outline_color_map)\n",
    "                outline_colors = outline_colors.fillna(rgba_to_hex(default_outline_color))\n",
    "                \n",
    "                vector_gdf.plot(ax=ax, color=fill_colors, edgecolor=outline_colors, linewidth=default_line_width)\n",
    "            \n",
    "\n",
    "        elif style_info.get('renderer_type') == 'graduatedSymbol':\n",
    "            print(\"Applying graduated style...\")\n",
    "            attribute_name = style_info.get('attribute')\n",
    "            if attribute_name not in vector_gdf.columns:\n",
    "                print(f\"Error: Attribute column '{attribute_name}' not found. Applying default style.\")\n",
    "                vector_gdf.plot(ax=ax,\n",
    "                                color=rgba_to_hex(default_fill_color),\n",
    "                                edgecolor=rgba_to_hex(default_outline_color),\n",
    "                                linewidth=default_line_width)\n",
    "            else:\n",
    "                fill_colors = []\n",
    "                for _, row in vector_gdf.iterrows():\n",
    "                    val = row[attribute_name]\n",
    "                    found_color = default_fill_color\n",
    "                    for cls in style_info.get('classes', []):\n",
    "                        if cls.get('lower_bound') is not None and cls.get('upper_bound') is not None:\n",
    "                            if cls['lower_bound'] <= val < cls['upper_bound']:\n",
    "                                found_color = cls.get('fill_color', default_fill_color)\n",
    "                                break\n",
    "                    fill_colors.append(rgba_to_hex(found_color))\n",
    "                \n",
    "                vector_gdf.plot(ax=ax,\n",
    "                                color=fill_colors,\n",
    "                                edgecolor=rgba_to_hex(default_outline_color),\n",
    "                                linewidth=default_line_width)\n",
    "\n",
    "        elif style_info.get('renderer_type') == 'RuleRenderer':\n",
    "            print(\"Applying rule-based style...\")\n",
    "            fill_colors = []\n",
    "            for _, row in vector_gdf.iterrows():\n",
    "                assigned_color = default_fill_color\n",
    "                for rule in style_info.get('rules', []):\n",
    "                    try:\n",
    "                        attribute_name = rule['filter'].split(' ')[0].strip().strip('\"').strip(\"'\")\n",
    "                        if attribute_name in row and pd.eval(rule['filter'], local_dict={attribute_name: row[attribute_name]}):\n",
    "                            assigned_color = rule.get('fill_color', default_fill_color)\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue \n",
    "                fill_colors.append(rgba_to_hex(assigned_color))\n",
    "\n",
    "            vector_gdf.plot(ax=ax,\n",
    "                            color=fill_colors,\n",
    "                            edgecolor=rgba_to_hex(default_outline_color),\n",
    "                            linewidth=default_line_width)\n",
    "\n",
    "        else:\n",
    "            print(\"Applying default blue style.\")\n",
    "            vector_gdf.plot(ax=ax,\n",
    "                            color='lightblue',\n",
    "                            edgecolor=rgba_to_hex(default_outline_color),\n",
    "                            linewidth=default_line_width)\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        plt.savefig(output_path)\n",
    "        plt.close(fig)\n",
    "        print(f\"Thumbnail saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating vector thumbnail: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "577507d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_item(state,\n",
    "                         district,\n",
    "                         block,\n",
    "                         layer_name,\n",
    "                         layer_map_csv_path,\n",
    "                         layer_desc_csv_path,\n",
    "                         column_desc_csv_path,\n",
    "                         ):    \n",
    "    print(layer_map_csv_path)\n",
    "    print(layer_desc_csv_path)\n",
    "    print(column_desc_csv_path)\n",
    "    #1. read layer description\n",
    "    layer_description = read_layer_description(filepath=layer_desc_csv_path,\n",
    "                                               layer_name=layer_name)\n",
    "    \n",
    "    #2. get geoserver url parameters from the layer details\n",
    "    geoserver_workspace_name,geoserver_layer_name,style_file_url,layer_display_name,ee_layer_name,_ = \\\n",
    "        read_layer_mapping(layer_map_csv_path = layer_map_csv_path,\n",
    "                           district = district,\n",
    "                           block=block,\n",
    "                           layer_name=layer_name,\n",
    "                        #    start_year=start_year,\n",
    "                        #    end_year=end_year\n",
    "                           )\n",
    "    \n",
    "    print(f\"geoserver_workspace_name={geoserver_workspace_name}\")\n",
    "    print(f\"geoserver_layer_name={geoserver_layer_name}\")\n",
    "    print(f\"style file url = {style_file_url}\")\n",
    "\n",
    "    #3. generate geoserver url\n",
    "    geoserver_url = generate_vector_url(workspace=geoserver_workspace_name,\n",
    "                                        layer_name=geoserver_layer_name,\n",
    "                                        geoserver_base_url=GEOSERVER_BASE_URL)\n",
    "    \n",
    "    #4. create vector item\n",
    "    layer_title = layer_display_name\n",
    "    layer_id = f\"{state}_{district}_{block}_{layer_name}\"\n",
    "\n",
    "    vector_item,vector_data_gdf = create_vector_item(geoserver_url,                                                     \n",
    "                                                #  id=f\"{layer_name}_{block}\",\n",
    "                                                #  id=geoserver_layer_name,\n",
    "                                                    id=layer_id,\n",
    "                                                    layer_title=layer_title,\n",
    "                                                    layer_description = layer_description,\n",
    "                                                    column_desc_csv_path = column_desc_csv_path\n",
    "                                                    )\n",
    "    \n",
    "    #5. add vector data asset\n",
    "    vector_item = add_vector_data_asset(vector_item,\n",
    "                                        geoserver_url=geoserver_url)\n",
    "    \n",
    "    #6. add table extension\n",
    "    vector_item = add_tabular_extension(vector_item=vector_item,\n",
    "                                        vector_data_gdf=vector_data_gdf,\n",
    "                                        column_desc_csv_path=column_desc_csv_path,\n",
    "                                        ee_layer_name=ee_layer_name)\n",
    "    \n",
    "    #7. add style file asset\n",
    "    add_stylefile_asset(STAC_item=vector_item,\n",
    "                        style_file_url=style_file_url\n",
    "                        )\n",
    "    \n",
    "    #start from here : TODO\n",
    "    #8. generate thumbnail\n",
    "    # if (start_year != ''):\n",
    "    #     thumbnail_filename = f'{block}_{layer_name}_{start_year}.png'\n",
    "    # else:\n",
    "    thumbnail_filename = f'{block}_{layer_name}.png' #TODO:\n",
    "    THUMBNAIL_PATH = os.path.join(THUMBNAIL_DIR,\n",
    "                                  thumbnail_filename)\n",
    "    \n",
    "    generate_vector_thumbnail(vector_gdf=vector_data_gdf,\n",
    "                              style_file_url=style_file_url,\n",
    "                              output_path=THUMBNAIL_PATH,\n",
    "                              STYLE_FILE_DIR=STYLE_FILE_DIR\n",
    "                              )\n",
    "    \n",
    "    #9. add thumbnail asset\n",
    "    vector_item = add_thumbnail_asset(\n",
    "        STAC_item=vector_item,\n",
    "        THUMBNAIL_PATH=THUMBNAIL_PATH,\n",
    "        LOCAL_DATA_DIR=LOCAL_DATA_DIR,\n",
    "        GITHUB_DATA_URL=GITHUB_DATA_URL\n",
    "    )\n",
    "\n",
    "    return vector_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b9c0c",
   "metadata": {},
   "source": [
    "Making a combined function to generate raster/vector STAC : \n",
    "The function generates the item, and updates the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d418c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_stac(state,\n",
    "                         district,\n",
    "                         block,\n",
    "                         layer_name,\n",
    "                         layer_map_csv_path='../data/input/metadata/layer_mapping.csv',\n",
    "                         layer_desc_csv_path='../data/input/metadata/layer_descriptions.csv',\n",
    "                         column_desc_csv_path='../data/input/metadata/vector_column_descriptions.csv'):\n",
    "    # print(layer_map_csv_path)\n",
    "    \n",
    "    vector_item = generate_vector_item(state,\n",
    "                                       district,\n",
    "                                       block,\n",
    "                                       layer_name,\n",
    "                                       layer_map_csv_path,\n",
    "                                       layer_desc_csv_path,\n",
    "                                       column_desc_csv_path)\n",
    "    \n",
    "    layer_STAC_generated = update_STAC_files(state,\n",
    "                                             district,\n",
    "                                             block,\n",
    "                                             STAC_item=vector_item)\n",
    "    \n",
    "    return layer_STAC_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e61332f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raster_stac(state,\n",
    "                         district,\n",
    "                         block,\n",
    "                         layer_name,\n",
    "                         layer_map_csv_path='../data/input/metadata/layer_mapping.csv',\n",
    "                         layer_desc_csv_path='../data/input/metadata/layer_descriptions.csv',\n",
    "                         start_year='',\n",
    "                         end_year=''):\n",
    "    \n",
    "    raster_item = generate_raster_item(state,\n",
    "                                       district,\n",
    "                                       block,\n",
    "                                       layer_name,\n",
    "                                       layer_map_csv_path,\n",
    "                                       layer_desc_csv_path,\n",
    "                                       start_year,\n",
    "                                       end_year)\n",
    "    \n",
    "    layer_STAC_generated = update_STAC_files(state,\n",
    "                                             district,\n",
    "                                             block,\n",
    "                                             STAC_item=raster_item)\n",
    "    \n",
    "    return layer_STAC_generated\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e9b87",
   "metadata": {},
   "source": [
    "Test the raster and vector flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17af36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block</th>\n",
       "      <th>district</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gobindpur</td>\n",
       "      <td>saraikela-kharsawan</td>\n",
       "      <td>jharkhand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mirzapur</td>\n",
       "      <td>mirzapur</td>\n",
       "      <td>uttar_pradesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>koraput</td>\n",
       "      <td>koraput</td>\n",
       "      <td>odisha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>badlapur</td>\n",
       "      <td>jaunpur</td>\n",
       "      <td>uttar_pradesh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       block             district          state\n",
       "0  gobindpur  saraikela-kharsawan      jharkhand\n",
       "1   mirzapur             mirzapur  uttar_pradesh\n",
       "2    koraput              koraput         odisha\n",
       "3   badlapur              jaunpur  uttar_pradesh"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block_district_state_df = pd.DataFrame({\n",
    "#     'block' : ['gobindpur','mirzapur','koraput','badlapur'],\n",
    "#     'district' : ['saraikela-kharsawan','mirzapur','koraput','jaunpur'],\n",
    "#     'state' : ['jharkhand','uttar_pradesh','odisha','uttar_pradesh']\n",
    "# })\n",
    "\n",
    "# block_district_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uttar_pradesh jaunpur badlapur\n"
     ]
    }
   ],
   "source": [
    "# block = 'badlapur'\n",
    "# district = block_district_state_df[block_district_state_df['block'] == block]['district'].iloc[0]\n",
    "# state = block_district_state_df[block_district_state_df['block'] == block]['state'].iloc[0]\n",
    "# print(state,district,block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4943444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_vector_stac(state=state,\n",
    "#                      district=district,\n",
    "#                      block=block,\n",
    "#                      layer_name='drainage_lines_vector',\n",
    "#                     #  layer_map_csv_path='../data/input/metadata/layer_mapping.csv',\n",
    "#                     #  layer_desc_csv_path='../data/input/metadata/layer_descriptions.csv',\n",
    "#                     #  column_desc_csv_path='../data/input/metadata/vector_column_descriptions.csv'\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6023802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_vector_stac(state=state,\n",
    "#                      district=district,\n",
    "#                      block=block,\n",
    "#                      layer_name='aquifer_vector',\n",
    "#                      # column_desc_csv_path='../data/input/metadata/vector_column_descriptions.csv',\n",
    "#                      # layer_map_csv_path='../data/input/metadata/layer_mapping.csv',\n",
    "#                      # layer_desc_csv_path='../data/input/metadata/layer_descriptions.csv',\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9608496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geoserver_workspace_name=canopy_height\n",
      "geoserver_layer_name=tree_health_ch_raster_jaunpur_badlapur_2021\n",
      "style file url = https://raw.githubusercontent.com/core-stack-org/QGIS-Styles/main/Tree%20Health/tree_cover_change.qml\n",
      "Raster URL: https://geoserver.core-stack.org:8443/geoserver/canopy_height/wcs?service=WCS&version=2.0.1&request=GetCoverage&CoverageId=canopy_height:tree_health_ch_raster_jaunpur_badlapur_2021&format=geotiff\n",
      "start_date = 2021-07-01 00:00:00\n",
      "end_date = 2022-06-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirzaree/miniconda3/envs/.stac/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'geoserver.core-stack.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created block catalog\n",
      "created district catalog\n",
      "created state collection\n",
      "created root catalog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_raster_stac(state=state,\n",
    "#                      district=district,\n",
    "#                      block=block,\n",
    "#                      layer_name='tree_canopy_height_raster',\n",
    "#                     #  layer_map_csv_path='../data/input/metadata/layer_mapping.csv',\n",
    "#                     #  layer_desc_csv_path='../data/input/metadata/layer_descriptions.csv',\n",
    "#                      start_year='2021'\n",
    "#                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".stac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
